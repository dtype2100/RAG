{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://api.python.langchain.com/en/latest/community/chat_models/langchain_community.chat_models.llamacpp.ChatLlamaCpp.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from langchain_community.chat_models import ChatLlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "from dotenv import dotenv_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dotenv_values(\n",
    "    dotenv_path = \"../.env\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatLlamaCpp(\n",
    "    temperature=0.5,\n",
    "    model_path=config[\"EXAONE-3.5-2.4B-Instruct-Q4_K_M.gguf\"],\n",
    "    n_ctx=10000,\n",
    "    n_gpu_layers=8,\n",
    "    n_batch=300,\n",
    "    max_tokens=512,\n",
    "    n_threads=multiprocessing.cpu_count() - 1,\n",
    "    repeat_penalty=1.5,\n",
    "    top_p=0.5,\n",
    "    verbose=True,\n",
    "    callback_manager=callback_manager,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트 구성\n",
    "question = \"LLM RAG시스템에 대해 알려줘\"\n",
    "prompt = f\"<|im_start|>system\\n당신은 도움이 되는 AI 어시스턴트입니다. 모든 답변은 한국어로 하세요. 질문에 대해 2~3문장으로 정확하고 유용한 답변을 제공해주세요.<|im_end|>\\n<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM RAG (Retrieval-Augmented Generation) 시스템은 대규모 언어 모델(Large Language Model, LLMs)을 기반으로 하며 주요 특징과 목적이 다음과 같습니다:\n",
      "\n",
      "1. **지식 검색 능력** : RAG는 주어진 텍스트나 질문에서 관련 정보를 빠르게 추출할 수 있도록 설계되었다. 이를 통해 사용자에겐 더욱 정확하고 맥락적인 답변 제공 가능하다."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    2209.66 ms\n",
      "llama_perf_context_print: prompt eval time =    2209.42 ms /   106 tokens (   20.84 ms per token,    47.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =   11878.34 ms /    91 runs   (  130.53 ms per token,     7.66 tokens per second)\n",
      "llama_perf_context_print:       total time =   14335.49 ms /   197 tokens\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
