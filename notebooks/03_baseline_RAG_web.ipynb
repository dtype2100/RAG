{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from langchain_community.chat_models import ChatLlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# RAG 체인 구성\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from dotenv import dotenv_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dotenv_values(\n",
    "    dotenv_path = \"../.env\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (16384) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    }
   ],
   "source": [
    "# RAG 시스템을 위한 LLM 설정 (KV 캐시 최적화 포함)\n",
    "llm = ChatLlamaCpp(\n",
    "    temperature=0.1,  # RAG에서는 더 일관된 답변을 위해 낮은 temperature 사용\n",
    "    model_path=config[\"EXAONE-3.5-2.4B-Instruct-Q4_K_M.gguf\"],\n",
    "    n_ctx=16384,  # 컨텍스트 크기를 적절히 조정 (32768은 메모리 부담이 큼)\n",
    "    n_gpu_layers=8,\n",
    "    n_batch=32,  # KV 캐시 최적화를 위해 더 작은 배치 크기\n",
    "    max_tokens=512,  # 더 긴 답변을 위해 토큰 수 증가\n",
    "    n_threads=multiprocessing.cpu_count() - 1,\n",
    "    repeat_penalty=1.1,  # RAG에서는 적당한 반복 패널티\n",
    "    top_p=0.9,  # RAG에서는 더 높은 top_p로 다양한 답변 생성\n",
    "    verbose=False,  # RAG에서는 verbose 비활성화로 출력 정리\n",
    "    callback_manager=callback_manager,\n",
    "    \n",
    "    # KV 캐시 최적화를 위한 설정들\n",
    "    use_mlock=True,  # 메모리 잠금으로 성능 향상\n",
    "    use_mmap=True,   # 메모리 맵핑 사용\n",
    "    \n",
    "    # 추가적인 KV 캐시 관련 설정들\n",
    "    # n_keep=-1,     # 모든 토큰을 캐시에 유지 (선택사항)\n",
    "    # rope_scaling_type=1,  # RoPE 스케일링 (긴 컨텍스트용)\n",
    "    # rope_freq_base=10000,  # RoPE 주파수 기본값\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "C:\\Users\\jw160\\AppData\\Local\\Temp\\ipykernel_20216\\3102760428.py:42: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"query\": query})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제시된 텍스트는 특정 뉴스 기사의 요약이나 일부"
     ]
    }
   ],
   "source": [
    "# 웹사이트 내용을 읽어서 답변하는 RAG 파이프라인 예시\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# 웹사이트에서 문서 로드\n",
    "url = \"https://n.news.naver.com/article/662/0000072581?cds=news_media_pc\"\n",
    "loader = WebBaseLoader(url)\n",
    "docs = loader.load()\n",
    "\n",
    "# 문서 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# 임베딩 및 벡터스토어 생성\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"jhgan/ko-sroberta-multitask\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=config[\"multilingual-e5-small-ko\"],\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.from_documents(splits, embeddings)\n",
    "\n",
    "# Retriever 생성\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# LLM 준비 (아래 llm은 기존 코드에서 정의된 ChatLlamaCpp 인스턴스 사용)\n",
    "# llm = ChatLlamaCpp(...)\n",
    "\n",
    "# RAG QA 체인 생성\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# 질문 예시\n",
    "query = \"이 뉴스의 주요 내용이 뭐야?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "print(\"답변:\", result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
