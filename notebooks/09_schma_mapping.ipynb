{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041115ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from typing import TypedDict, List\n",
    "from langgraph.graph import StateGraph, END\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# âœ… [ë³€ê²½ 1] llama_cpp ëŒ€ì‹  langchain_ollama ì‚¬ìš©\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# =============================================================================\n",
    "# [ì„¤ì •] \n",
    "# =============================================================================\n",
    "# ê²½ë¡œ ëŒ€ì‹  'ëª¨ë¸ ì´ë¦„'ë§Œ ì•Œë©´ ë©ë‹ˆë‹¤.\n",
    "MODEL_NAME = \"qwen2.5-coder:3b\" \n",
    "\n",
    "TARGET_SCHEMA = [\n",
    "    \"user_id\", \"user_name\", \"phone_number\", \"email_address\", \n",
    "    \"signup_date\", \"last_login\", \"is_active\", \"shipping_address\"\n",
    "]\n",
    "\n",
    "# =============================================================================\n",
    "# [ì´ˆê¸°í™”] ëª¨ë¸ ì—°ê²°\n",
    "# =============================================================================\n",
    "print(\"â³ Loading Models...\")\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "target_embeddings = embed_model.encode(TARGET_SCHEMA)\n",
    "\n",
    "# âœ… [ë³€ê²½ 2] Ollama ê°ì²´ ìƒì„± (ë§¤ìš° ì‹¬í”Œí•¨)\n",
    "# format=\"json\" ì˜µì…˜ì„ ì¼œë©´ ëª¨ë¸ì´ ë¬´ì¡°ê±´ JSONë§Œ ë±‰ìœ¼ë ¤ê³  ë…¸ë ¥í•©ë‹ˆë‹¤. (ê°•ë ¥ ì¶”ì²œ)\n",
    "llm = ChatOllama(\n",
    "    model=MODEL_NAME,\n",
    "    temperature=0,      # ì‚¬ì‹¤ ê¸°ë°˜ì´ë¯€ë¡œ 0 ì¶”ì²œ\n",
    "    format=\"json\"       # âœ¨ Ollamaì˜ í‚¬ëŸ¬ ê¸°ëŠ¥ (JSON ê°•ì œí™”)\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# [í”„ë¡¬í”„íŠ¸]\n",
    "# =============================================================================\n",
    "mapping_template_str = \"\"\"\n",
    "You are a Data Engineer. Select the best Target Column from Candidates.\n",
    "[Input]\n",
    "- Source: \"{source_col}\"\n",
    "- Sample: \"{sample_value}\"\n",
    "- Candidates: {candidates}\n",
    "\n",
    "Analyze and select ONE target column.\n",
    "Return JSON ONLY.\n",
    "\n",
    "Example:\n",
    "{{ \"selected_column\": \"col_name\", \"reason\": \"why\" }}\n",
    "\"\"\"\n",
    "\n",
    "MAPPING_PROMPT = PromptTemplate(\n",
    "    template=mapping_template_str,\n",
    "    input_variables=[\"source_col\", \"sample_value\", \"candidates\"]\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# [Node Logic]\n",
    "# =============================================================================\n",
    "class MappingState(TypedDict):\n",
    "    source_col: str\n",
    "    sample_value: str\n",
    "    candidates: List[str]\n",
    "    final_mapping: str\n",
    "    reasoning: str\n",
    "\n",
    "def retriever_node(state: MappingState):\n",
    "    # (ì´ì „ê³¼ ë™ì¼í•˜ì—¬ ìƒëµ)\n",
    "    source_vec = embed_model.encode([state[\"source_col\"]])\n",
    "    similarities = cosine_similarity(source_vec, target_embeddings)[0]\n",
    "    top_k_indices = np.argsort(similarities)[-3:][::-1]\n",
    "    candidates = [TARGET_SCHEMA[i] for i in top_k_indices]\n",
    "    return {\"candidates\": candidates}\n",
    "\n",
    "def slm_reasoning_node(state: MappingState):\n",
    "    print(f\"ğŸ§  [SLM] Ollama ì¶”ë¡  ì‹œì‘ ({MODEL_NAME})...\")\n",
    "\n",
    "    # ì²´ì¸ ì—°ê²° (LCEL ë¬¸ë²• ì‚¬ìš© ê°€ëŠ¥)\n",
    "    # chain = Prompt | LLM\n",
    "    chain = MAPPING_PROMPT | llm\n",
    "\n",
    "    try:\n",
    "        # âœ… [ë³€ê²½ 3] í˜¸ì¶œ ë°©ì‹ (invoke)\n",
    "        # ChatOllamaëŠ” ê²°ê³¼ë¡œ AIMessage ê°ì²´ë¥¼ ì¤ë‹ˆë‹¤. .contentë¡œ ë‚´ìš©ì„ êº¼ëƒ…ë‹ˆë‹¤.\n",
    "        response_msg = chain.invoke({\n",
    "            \"source_col\": state[\"source_col\"],\n",
    "            \"sample_value\": state[\"sample_value\"],\n",
    "            \"candidates\": state[\"candidates\"]\n",
    "        })\n",
    "        \n",
    "        response_text = response_msg.content\n",
    "        \n",
    "        # Ollamaê°€ format=\"json\"ì´ë©´ ëŒ€ë¶€ë¶„ ê¹”ë”í•œ JSONì´ ë‚˜ì˜µë‹ˆë‹¤.\n",
    "        result_json = json.loads(response_text)\n",
    "        final_col = result_json.get(\"selected_column\", \"Unknown\")\n",
    "        reason = result_json.get(\"reason\", \"No reason\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        final_col = \"Error\"\n",
    "        reason = str(e)\n",
    "\n",
    "    return {\"final_mapping\": final_col, \"reasoning\": reason}\n",
    "\n",
    "# =============================================================================\n",
    "# [Main ì‹¤í–‰]\n",
    "# =============================================================================\n",
    "# (ê·¸ë˜í”„ ì •ì˜ ë° ì‹¤í–‰ ì½”ë“œëŠ” ì´ì „ê³¼ ë™ì¼)\n",
    "workflow = StateGraph(MappingState)\n",
    "workflow.add_node(\"retriever\", retriever_node)\n",
    "workflow.add_node(\"reasoner\", slm_reasoning_node)\n",
    "workflow.set_entry_point(\"retriever\")\n",
    "workflow.add_edge(\"retriever\", \"reasoner\")\n",
    "workflow.add_edge(\"reasoner\", END)\n",
    "app = workflow.compile()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = app.invoke({\n",
    "        \"source_col\": \"m_hp\", \n",
    "        \"sample_value\": \"010-1234-5678\"\n",
    "    })\n",
    "    print(f\"\\nâœ… Result: {result['final_mapping']} ({result['reasoning']})\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
